{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test BERT model\n",
    "#### Code borrowed from NLP Lab 8 (Prud'hommeaux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GQ5o9DE_Cyk"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCbhQNSR_oaJ"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJLvG_tePEjy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oc2DV6ylSe88"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/nlp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vd3KiryrP-AA"
   },
   "outputs": [],
   "source": [
    "# Read in list of all line numbers selected for cluster test set \n",
    "\n",
    "test_cluster_lines = []\n",
    "with open(path + 'test_cluster_lines', 'r') as clusterlines:\n",
    "    test_cluster_lines = clusterlines.readlines()[0].split(',')[:-1]\n",
    "    test_cluster_lines = [int(i) for i in test_cluster_lines]\n",
    "\n",
    "drop_lines = test_cluster_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtmGkAA0H4KW"
   },
   "outputs": [],
   "source": [
    "# Drop lines in cluster test set from the rest of the data\n",
    "\n",
    "train_raw = pd.read_csv(path + 'sample_subtitles_data.csv')\n",
    "train_raw.drop(drop_lines,axis=0,inplace=True)\n",
    "train_raw.drop(\"index\",axis=1,inplace=True)\n",
    "train_raw.to_csv(path + 'train.csv', index=None)\n",
    "\n",
    "print(train_raw.columns)\n",
    "print(len(train_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UgqD1-VGplu"
   },
   "outputs": [],
   "source": [
    "# Convert decade labels to unique integer codes\n",
    "\n",
    "train_df = pd.read_csv(path + 'train.csv')\n",
    "train_df['label'] = train_df['label'].astype('category').cat.codes\n",
    "train_df.to_csv(path + 'train.csv', index=None)\n",
    "\n",
    "cluster_df = pd.read_csv(path + 'test_cluster.csv')\n",
    "cluster_df['label'] = cluster_df['label'].astype('category').cat.codes\n",
    "cluster_df.to_csv(path + 'test_cluster.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIdT7T6w_4Dm"
   },
   "outputs": [],
   "source": [
    "# Uses distilBERT\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tFO7kUOUBYJ"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function (pads and truncates lines in dataset)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfV8Y_adTki_"
   },
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "\n",
    "dataset = load_dataset('csv',data_files={'train': '/content/drive/MyDrive/nlp/train.csv', \n",
    "                                           'test_cluster': '/content/drive/MyDrive/nlp/test_cluster.csv'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFRN5PwRCgaZ"
   },
   "outputs": [],
   "source": [
    "# 80/20 train/test split\n",
    "\n",
    "train_test = dataset[\"train\"].shuffle(seed=42)\n",
    "train_dataset = train_test.select([i for i in range(500000,len(train_test))])  ## Training data (80% of total lines)\n",
    "test_random_dataset = train_test.select([i for i in range(500000)])   ## Randomly select lines for random test set (10% of total lines)\n",
    "\n",
    "test_cluster_dataset = dataset[\"test_cluster\"].shuffle(seed=42)  ## Pre-selected cluster test set (10% of total lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEuRDIZKCiY6"
   },
   "outputs": [],
   "source": [
    "# Remove any data with null lines\n",
    "\n",
    "train_dataset = train_dataset.filter(lambda x: x[\"text\"])\n",
    "test_cluster_dataset = test_cluster_dataset.filter(lambda x: x[\"text\"])\n",
    "test_random_dataset = test_random_dataset.filter(lambda x: x[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp-L-14_IaQx"
   },
   "outputs": [],
   "source": [
    "### CUT TRAIN AND TEST DATA TO SMALLER SAMPLES\n",
    "# Randomly sample 50,000 training lines and 5,000 test lines for each test set\n",
    "\n",
    "test_random_dataset = test_random_dataset.select([i for i in range(5000)])\n",
    "test_cluster_dataset = test_cluster_dataset.select([i for i in range(5000)])\n",
    "train_dataset = train_dataset.select([i for i in range(50000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKwTkzBtQeDW"
   },
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTmiNdosVfFD"
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing function to data\n",
    "\n",
    "train = train_dataset.map(preprocess_function)\n",
    "test_cluster = test_cluster_dataset.map(preprocess_function)\n",
    "test_random = test_random_dataset.map(preprocess_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mRWZ1139N5J"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXlj8sQI9wES"
   },
   "outputs": [],
   "source": [
    "# Define function to report performance metrics (could not use load_metric()\n",
    "# because data is multi-class)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "    # load_accuracy = load_metric(\"accuracy\", average='micro')\n",
    "    # load_precision = load_metric(\"precision\", average='micro')\n",
    "    # load_recall = load_metric(\"recall\", average='micro')\n",
    "    # load_f1 = load_metric(\"f1\", average='micro')\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # accuracy = load_accuracy.predict(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # precision = load_precision.predict(predictions=predictions, references=labels)[\"precision\"]\n",
    "    # recall = load_recall.predict(predictions=predictions, references=labels)[\"recall\"]\n",
    "    # f1 = load_f1.predict(predictions=predictions, references=labels)[\"f1\"]\n",
    "    print(classification_report(labels, predictions))\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions,average='micro')\n",
    "    recall = recall_score(labels, predictions,average='micro')\n",
    "    f1 = f1_score(labels, predictions,average='micro')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvjyDD_3-n1h"
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= path + \"results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test_random,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcK0XOk2_3QU"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9bM0eN3ASc3"
   },
   "outputs": [],
   "source": [
    "# Test model on random test set\n",
    "\n",
    "trainer.evaluate(eval_dataset=test_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yso_ve8lljsj"
   },
   "outputs": [],
   "source": [
    "# Test model on cluster test set\n",
    "\n",
    "trainer.evaluate(eval_dataset=test_cluster)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
