{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split \n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Word2Vec(corpus_file='tok_subtitles_data', vector_size=100, window=5, min_count=3, workers=4)\n",
    "    print(\"Model built.\")\n",
    "    model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decades = [\"1930s\", \"1940s\", \"1950s\", \"1960s\", \"1970s\", \"1980s\", \"1990s\", \"2000s\", \"2010s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "7300000\n",
      "7400000\n",
      "7500000\n",
      "7600000\n",
      "7700000\n",
      "7800000\n",
      "7900000\n",
      "8000000\n",
      "8100000\n",
      "8200000\n",
      "8300000\n",
      "8400000\n",
      "8500000\n",
      "8600000\n",
      "8700000\n",
      "8800000\n",
      "8900000\n",
      "9000000\n",
      "9100000\n",
      "9200000\n",
      "9300000\n",
      "9400000\n",
      "9500000\n",
      "9600000\n",
      "9700000\n",
      "9800000\n",
      "9900000\n",
      "10000000\n",
      "10100000\n",
      "10200000\n",
      "10300000\n",
      "10400000\n",
      "10500000\n",
      "10600000\n",
      "10700000\n",
      "10800000\n",
      "10900000\n",
      "11000000\n",
      "11100000\n",
      "11200000\n",
      "11300000\n",
      "11400000\n",
      "11500000\n",
      "11600000\n",
      "11700000\n",
      "11800000\n",
      "11900000\n",
      "12000000\n",
      "12100000\n",
      "12200000\n",
      "12300000\n",
      "12400000\n",
      "12500000\n",
      "12600000\n",
      "12700000\n",
      "12800000\n",
      "12900000\n",
      "13000000\n",
      "13100000\n",
      "13200000\n",
      "13300000\n",
      "13400000\n",
      "13500000\n",
      "13600000\n",
      "13700000\n",
      "13800000\n",
      "13900000\n",
      "14000000\n",
      "14100000\n",
      "14200000\n",
      "14300000\n",
      "14400000\n",
      "14500000\n",
      "14600000\n",
      "14700000\n",
      "14800000\n",
      "14900000\n",
      "15000000\n",
      "15100000\n",
      "15200000\n",
      "15300000\n",
      "15400000\n",
      "15500000\n",
      "15600000\n",
      "15700000\n",
      "15800000\n",
      "15900000\n",
      "16000000\n",
      "16100000\n",
      "16200000\n",
      "16300000\n",
      "16400000\n",
      "16500000\n"
     ]
    }
   ],
   "source": [
    "allmovies = []\n",
    "current = []\n",
    "movie_indices = []\n",
    "  \n",
    "count = 0\n",
    "with open('tok_movie_data', 'r') as tok_movies, open('subtitles_data', 'r') as subs:\n",
    "    subs.readline()\n",
    "    while True:\n",
    "        sub_line = subs.readline()\n",
    "        line = tok_movies.readline()\n",
    "        if not (line or sub_line):\n",
    "            allmovies.append(current)\n",
    "            break\n",
    "        match = re.search(r\"^\\*\\*\\*[0-9]+\\,[0-9]+\\,[0-9]+\\*\\*\\*$\", sub_line)\n",
    "        if match:\n",
    "            allmovies.append(current)\n",
    "            current = []\n",
    "            count += 1\n",
    "            movie_indices.append(count)\n",
    "            sub_line = subs.readline()\n",
    "            continue\n",
    "        current.append(line)\n",
    "        count += 1\n",
    "        if (count%100000) == 0:\n",
    "            print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(allmovies))\n",
    "# print(allmovies[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# movie_ids = []\n",
    "# movie_decades = []\n",
    "# movie_titles = []\n",
    "# movie_genres = []\n",
    "# movie_years = []\n",
    "\n",
    "# metadata = pd.read_excel('movie_metadata.xlsx', sheet_name='Sheet1')\n",
    "# df = pd.DataFrame(metadata, columns=['textID','title','year','decade','genre'])\n",
    "\n",
    "# movie_ids = df['textID'].tolist()\n",
    "# movie_decades = df['decade'].tolist()\n",
    "# movie_titles = df['title'].tolist()\n",
    "# movie_genres = df['genre'].tolist()\n",
    "# movie_years = df['year'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# sample_movie_indices = []\n",
    "# sample_metadata = []\n",
    "\n",
    "# for decade in decades:\n",
    "#     for i in range(450):\n",
    "#         index = random.randint(0, (len(movie_decades)-1))\n",
    "#         while (movie_decades[index] != decade) | (index in sample_movie_indices):\n",
    "#             index = random.randint(0, (len(movie_decades)-1))\n",
    "#         sample_movie_indices.append(index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_movie_indices = []\n",
    "sample_metadata = []\n",
    "\n",
    "sample_file = pd.read_excel(\"sample_movie_metadata.xlsx\", sheet_name='Sample')\n",
    "sample_df = pd.DataFrame(sample_file, columns=['index','textID','title','year','decade','genre'])\n",
    "\n",
    "sample_movie_indices = sample_df['index'].tolist()\n",
    "sample_movie_indices = [int(i) for i in sample_movie_indices]\n",
    "\n",
    "ids = sample_df['textID'].tolist()\n",
    "titles = sample_df['title'].tolist()\n",
    "decades = sample_df['decade'].tolist()\n",
    "years = sample_df['year'].tolist()\n",
    "genres = sample_df['genre'].tolist()\n",
    "\n",
    "for i in range(len(sample_movie_indices)):\n",
    "    index = sample_movie_indices[i]\n",
    "    id = ids[i]\n",
    "    title = titles[i]\n",
    "    decade = decades[i]\n",
    "    year = years[i]\n",
    "    genre = genres[i]\n",
    "    meta = [index, id, title, year, decade, genre]\n",
    "    sample_metadata.append(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4050\n",
      "[168, 25161, 'Gambling Lady', 1934, '1930s', 'Drama']\n"
     ]
    }
   ],
   "source": [
    "print(len(sample_movie_indices))\n",
    "print(sample_metadata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movievectors = []\n",
    "\n",
    "# for i in range(len(sample_movie_indices)):\n",
    "#     index = sample_movie_indices[i]\n",
    "#     movie = allmovies[index]\n",
    "#     totvec = np.zeros(100)\n",
    "#     for line in movie:\n",
    "#         for token in line.split(\" \"):\n",
    "#             try:\n",
    "#                 totvec = totvec + model.wv[token]\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "#     if (i%100 == 0):\n",
    "#         print(str(i))\n",
    "\n",
    "#     movievectors.append(totvec)\n",
    "\n",
    "#     id = movie_ids[index]\n",
    "#     title = movie_titles[index]\n",
    "#     decade = movie_decades[index]\n",
    "#     year = movie_years[index]\n",
    "#     genre = movie_genres[index]\n",
    "#     meta = [index, id, title, year, decade, genre]\n",
    "#     sample_metadata.append(meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(movievectors))\n",
    "# print(movievectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sample_meta():\n",
    "    sample_df = pd.DataFrame(sample_metadata, columns=['index','textID','title','year','decade','genre'])\n",
    "    sample_df.to_excel(\"sample_movie_metadata.xlsx\", sheet_name='Sample', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_movie_vectors():\n",
    "    with open('movie_vectors', 'w') as f:\n",
    "        for vector in movievectors:\n",
    "            f.write(str(vector) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movievectors = []\n",
    "with open('movie_vectors', 'r') as fread:\n",
    "    file = fread.read().rstrip()\n",
    "    vectors = file.replace(\"[\", \"\").strip().split(']')\n",
    "    for i in range(4050):\n",
    "        current = np.zeros(100)\n",
    "        values = vectors[i].split()\n",
    "        for j in range(100):\n",
    "            current[j] = current[j] + float(values[j].strip('\\n'))\n",
    "        movievectors.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -7758.42975128   1437.61948495   6363.02029613   -353.85453451\n",
      "   2526.86870275  -8162.21084991    804.71707907    175.11357649\n",
      " -15940.49886915  -2555.8843161   -6439.0239977   -9985.90603238\n",
      "  -6357.20584343   4104.28803649  10729.73373431  -7989.6275671\n",
      "  -5581.82473803   1534.17248715  -6567.51363221  -7116.25713211\n",
      "   7299.46573964  -1040.08630491  -6500.79773156   1147.78441579\n",
      "     73.42421195  -2476.93878697   2882.9242971    1643.78764048\n",
      " -16911.81955129 -13905.92811035  10349.69969014   2368.63109985\n",
      "  -2945.86452826 -10079.32786384  -1706.96660779  -7064.21037742\n",
      "  -7724.46642654  -8832.71270214   -509.67079343 -13050.17396462\n",
      "  -7090.40294494    404.70277117  -4650.73009597  -2812.97976232\n",
      "   5304.01430519   1069.29514702  -4753.13754769  -8184.67936857\n",
      "  -5883.52933817   -640.19291059  -1490.83648687  -3770.9354938\n",
      "   9709.18187279   6822.22604097    965.81159874  -7461.56350547\n",
      "   9915.12953046  -6181.30518893   2706.79287654   6309.48009577\n",
      "   2422.38295088  -4956.00426438  -1961.46788636  -8468.49071047\n",
      "  -6090.41146249   -408.25648846  -3547.66805711   2945.24140088\n",
      "   5595.76270733  14615.82010443  -3276.45398287  -2237.81736849\n",
      "  11140.34566214  -7845.92420332   8397.69875508   9990.4293606\n",
      "   2066.3582458   -5166.92386236  -9116.42033673 -12373.63565384\n",
      "  -1059.08609974   7075.34334987  -9314.98240286    558.74238066\n",
      "  -2993.7346257    4627.30978985  -2096.66821539  -7978.78626038\n",
      "   5829.32322526  -5037.21403058   6495.27251428   1817.56387295\n",
      "   9982.0268697   -1975.4558382    9030.24943091   8989.8711251\n",
      "  -4494.97777101   4620.44735274  -4647.83001567  -4805.75955537]\n"
     ]
    }
   ],
   "source": [
    "print(movievectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 30\n",
    "kmmovies = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "movieclusters = kmmovies.fit_predict(movievectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[376, 30418, 'Marie Antoinette', 1938, '1930s', 'Biography, Drama, History']\n",
      "[425, 31725, 'Ninotchka', 1939, '1930s', 'Comedy, Romance']\n",
      "[444, 32080, 'Union Pacific', 1939, '1930s', 'Drama, Western']\n",
      "[423, 31679, 'Mr. Smith Goes to Washington', 1939, '1930s', 'Comedy, Drama']\n",
      "[479, 32484, 'Foreign Correspondent', 1940, '1940s', 'Action, Romance, Thriller']\n",
      "[971, 40242, 'Command Decision', 1948, '1940s', 'Action, Drama, War']\n",
      "[795, 37614, 'The Corn Is Green', 1945, '1940s', 'Drama']\n",
      "[688, 35966, 'Hangmen Also Die!', 1943, '1940s', 'Drama, Film-Noir, Thriller']\n",
      "[582, 34272, 'That Hamilton Woman', 1941, '1940s', 'Drama, History, Romance, War']\n",
      "[698, 36172, 'The More the Merrier', 1943, '1940s', 'Comedy, Romance, War']\n",
      "[1251, 44331, 'Affair in Trinidad', 1952, '1950s', 'Crime, Drama, Film-Noir']\n",
      "[1826, 53291, 'Some Like It Hot', 1959, '1950s', 'Comedy, Romance']\n",
      "[1556, 48947, 'Anastasia', 1956, '1950s', 'Biography, Drama, History']\n",
      "[1150, 42648, 'Kiss Tomorrow Goodbye', 1950, '1950s', 'Crime, Film-Noir, Thriller']\n",
      "[1117, 42256, 'The Black Rose', 1950, '1950s', 'Adventure, History, Romance']\n",
      "[1254, 44391, 'The Bad and the Beautiful', 1952, '1950s', 'Drama, Romance']\n",
      "[1417, 46912, 'Dial M for Murder', 1954, '1950s', 'Crime, Thriller']\n",
      "[1806, 52933, 'It Happened to Jane', 1959, '1950s', 'Comedy']\n",
      "[1391, 46463, 'Trouble Along the Way', 1953, '1950s', 'Comedy, Drama, Romance']\n",
      "[1381, 46280, 'Scared Stiff', 1953, '1950s', 'Comedy, Horror, Musical']\n",
      "[1200, 43459, 'Decision Before Dawn', 1951, '1950s', 'Drama, War']\n",
      "[1358, 45963, 'Kiss Me Kate', 1953, '1950s', 'Comedy, Musical, Romance']\n",
      "[1852, 53804, 'Exodus', 1960, '1960s', 'Action, Drama, History']\n",
      "[1970, 56193, 'Lolita', 1962, '1960s', 'Crime, Drama, Romance']\n",
      "[2062, 57887, 'Bikini Beach', 1964, '1960s', 'Comedy, Musical, Romance']\n",
      "[2015, 56956, \"The Courtship of Eddie's Father\", 1963, '1960s', 'Comedy, Drama, Family']\n",
      "[2262, 62430, 'Valley of the Dolls', 1967, '1960s', 'Drama, Music, Romance']\n",
      "[2140, 59431, 'Marriage on the Rocks', 1965, '1960s', 'Comedy']\n",
      "[1923, 55256, 'One, Two, Three', 1961, '1960s', 'Comedy']\n",
      "[1884, 54345, 'Strangers When We Meet', 1960, '1960s', 'Drama, Romance']\n",
      "[2132, 59250, 'The Hallelujah Trail', 1965, '1960s', 'Comedy, Western']\n",
      "[2188, 60479, 'The Group', 1966, '1960s', 'Drama']\n",
      "[2071, 58092, 'Father Goose', 1964, '1960s', 'Adventure, Comedy, Romance']\n",
      "[1841, 53604, 'The Apartment', 1960, '1960s', 'Comedy, Drama, Romance']\n",
      "[1998, 56626, 'Two for the Seesaw', 1962, '1960s', 'Drama, Romance']\n",
      "[1854, 53841, 'From the Terrace', 1960, '1960s', 'Drama, Romance']\n",
      "[1882, 54331, 'Spartacus', 1960, '1960s', 'Adventure, Biography, Drama, History, War']\n",
      "[1977, 56255, 'Mr. Hobbs Takes a Vacation', 1962, '1960s', 'Comedy, Family']\n",
      "[1946, 55728, 'Advise & Consent', 1962, '1960s', 'Drama, Thriller']\n",
      "[2800, 75223, 'Silver Streak', 1976, '1970s', 'Action, Comedy, Crime, Romance, Thriller']\n",
      "[2679, 72251, 'The Taking of Pelham One Two Three', 1974, '1970s', 'Action, Crime, Thriller']\n",
      "[2471, 67402, 'Mary, Queen of Scots', 1971, '1970s', 'Biography, Drama, History']\n",
      "[2415, 66301, 'Rio Lobo', 1970, '1970s', 'Adventure, Romance, War']\n",
      "[2599, 70290, 'The Last Detail', 1973, '1970s', 'Comedy, Drama']\n",
      "[2890, 77289, 'California Suite', 1978, '1970s', 'Comedy, Drama, Romance']\n",
      "[2886, 77248, 'Blue Collar', 1978, '1970s', 'Crime, Drama']\n",
      "[3339, 87003, 'Broadway Danny Rose', 1984, '1980s', 'Comedy']\n",
      "[4860, 112384, 'Apollo 13', 1995, '1990s', 'Adventure, Drama, History']\n",
      "[4406, 103859, 'Boomerang', 1992, '1990s', 'Comedy, Drama, Romance']\n",
      "[4839, 114594, 'Swimming with Sharks', 1994, '1990s', 'Comedy, Crime']\n",
      "[8510, 1182887, 'Flirting with Forty', 2008, '2000s', 'Comedy, Drama, Romance']\n",
      "[6611, 310910, 'Confidence', 2003, '2000s', 'Crime, Thriller']\n",
      "[7933, 396555, 'Meet the Robinsons', 2007, '2000s', 'Animation, Adventure, Comedy']\n",
      "[8490, 1152758, 'Dear Zachary: A Letter to a Son About His Father', 2008, '2000s', 'Documentary, Biography, Crime']\n",
      "[6830, 327247, 'The Whole Ten Yards', 2004, '2000s', 'Comedy, Crime, Thriller']\n",
      "[10876, 837562, 'Hotel Transylvania', 2012, '2010s', 'Animation, Comedy, Family']\n",
      "[11823, 2557490, 'A Million Ways to Die in the West', 2014, '2010s', 'Action, Comedy, Western']\n",
      "[11032, 1801096, 'Sexy Evil Genius', 2013, '2010s', 'Comedy, Drama, Mystery']\n",
      "[10462, 1598828, 'One for the Money', 2012, '2010s', 'Action, Comedy, Crime, Romance, Thriller']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(movieclusters)):\n",
    "    if movieclusters[i] == 29:\n",
    "        print(sample_metadata[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_clusters():\n",
    "    with open(('clusters_' + str(num_clusters)), 'w') as f:\n",
    "        for i in range(num_clusters):\n",
    "            f.write(\"\\nCLUSTER {num}\\n\".format(num=i))\n",
    "            for j in range(len(movieclusters)):\n",
    "                if movieclusters[j] == i:\n",
    "                    f.write(str(sample_metadata[j]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movieclusterdistances = kmmovies.transform(movievectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances = np.zeros(num_clusters)\n",
    "# cluster_movies = [None]*num_clusters\n",
    "\n",
    "# for i in range(len(movieclusterdistances)):\n",
    "#     current = movieclusterdistances[i]\n",
    "#     for j in range(len(current)):\n",
    "#         if distances[j]:\n",
    "#             if current[j] < distances[j]:\n",
    "#                 distances[j] = current[j]\n",
    "#                 cluster_movies[j] = sample_metadata[i]\n",
    "#         else:\n",
    "#             distances[j] = current[j]\n",
    "#             cluster_movies[j] = sample_metadata[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(distances)\n",
    "# print(cluster_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_subtitles_data', 'w') as f:\n",
    "    for i in range(len(sample_movie_indices)):\n",
    "        index = sample_movie_indices[i]\n",
    "        movie = allmovies[index]\n",
    "        for line in movie:\n",
    "#             totvec = np.zeros(100)\n",
    "            f.write(line.strip('\\n').replace(\",\", \"\").strip() + ',' + decades[i] + ',' + str(i) + '\\n')\n",
    "#             for token in line.split(\" \"):\n",
    "#                 try:\n",
    "#                     totvec = totvec + model.wv[token]\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "#             linevectors.append(totvec)\n",
    "#         if (i%100 == 0):\n",
    "#             print(str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load BERT and the preprocessing model from TF Hub.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bert_preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m bert_encoder \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#CREATING BERT MODELS \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "# ON COLAB, THESE SHOULD WORK TOTALLY FINE\n",
    "\n",
    "# Load BERT and the preprocessing model from TF Hub.\n",
    "bert_preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')\n",
    "bert_encoder = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3')\n",
    "\n",
    "#CREATING BERT MODELS \n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "#SPLIT DATA INTO TRAINING AND TEST SETS\n",
    "#data = whereever the subtitles are being read from \n",
    "dataframe = pd.DataFrame(data, columns=[year for year in decades])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split()\n",
    "\n",
    "\n",
    "#CONSTRUCT MODEL \n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[outputs])\n",
    "#pretty sure we now need to pass in a classification task - talk with Grace\n",
    "\n",
    "METRICS = [ \n",
    "    tf.keras.metrics.CategoricalAccuracy(name='Accuracy')\n",
    "    tf.keras.metrics.Precision(name='Precision')\n",
    "    tf.keras.metrics.Recall(name='Recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=METRICS)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import confusion_matrix, classification report \n",
    "# visualization for accuracy: This will probably have to be done by decade so not sure if itll work \n",
    "cm = confusion_matrix(Y_test, Y_predicted) \n",
    "sn.heatmap(cm, annot='true', fmt='d')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
